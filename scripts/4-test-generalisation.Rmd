---
title: "4) Test Generalisation"
author: "Emily Williams"
output: html_document
---

NOTE:

For these scripts to work, you must follow the 3-step set-up instructions on the GitHub Repo's README: https://github.com/EA-Williams/The-Typability-Index/blob/main/README.md

This includes downloading Dhakal et al.'s (2018) 136M Keystrokes Dataset from https://userinterfaces.aalto.fi/136Mkeystrokes/ into the required folder.

These scripts are intended to be run chunk-by-chunk rather than knit.



NOTE: Within the code, the test of the model's generalisation to the separate dataset is referred to as 'cross-validation', but it was later decided that 'generalisation' is a clearer term for this test.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE) 
```


## Load libraries

```{r load_libraries}

library(tidyverse)
library(here)
library(quanteda.textstats) # for syllable counting (more accurate than sylcount)
library(hunspell) # for spell check (is this word in the dictionary)
library(caret) # for model predictions

# function to split word/sentence into bigrams
str_split_bigrams = function(x) {
    substring(x, first = 1:(nchar(x) - 1), last = 2:nchar(x))
} 

# function to add the stated columns if they don't exist
add_cols <- function(df, cols) {
  add <- cols[!cols %in% names(df)]
  if (length(add) != 0 ) df[add] <- 0
  return(df)
}

# function to calculate the predictor variables
calculatePredictorVariables <- function(sentencesDF) {
  
  # wants only a 1d df with the first column as sentences
  
  options(dplyr.summarise.inform = FALSE)
  
  sentencesDF <- as.data.frame(sentencesDF) %>%
    rename(sentence = 1) %>%
    mutate(sentNum = row_number()) %>%
    select(sentNum, sentence)
  
# load in the helper data, e.g. bigram frequencies, hand categorisations
  
# load bigram frequencies
load(here("data", "predictor_calculation_aids", "GutenbergFrequencyTables.RData"))
rm(AllTrigramsMLE, AllLetters)

## calc percentile ranks of bigrams
AllBigramsMLE <- AllBigramsMLE %>%
  mutate(rank = rank(-Frequency, ties = "min"),
         # inflection point after first 15 top frequency bigrams in terms of their frequency (and frequency percent) - these bigrams account for 28.09% of all bigrams in English (in the corpus that Crump used)
        freqPerc = (Frequency / sum(Frequency)) * 100) %>%
  arrange(rank) %>%
  mutate(freqPercCumu = cumsum(freqPerc),
         top15bigram = if_else(rank <= 15, 1, 0))


# read in the keyboard info
keyboardInfo <- read_tsv(here("data", "predictor_calculation_aids", "keyboardInfo.txt"),
                         trim_ws = FALSE, quote = "", show_col_types = FALSE) %>%
  # for easier coding later, change the shifts to one-character symbols which are not keyboard characters in themselves
  mutate(character = case_when(character == "RSHIFT" ~ "ℛ",
                               character == "LSHIFT" ~ "ℒ",
                               TRUE ~ character))




print("Calculating the proportion of each character type (e.g. uppercase, numbers) for each sentence.")

# perform the simple calculations
data_1_simpleCalcs <- sentencesDF %>%
  # for each unique sentence, calculate these things
  mutate(numActualWords = str_count(sentence, "\\S+"),
         numChars = nchar(sentence),
         meanWordLength = numChars / numActualWords,
         # what proportion the average word takes up of the sentence
         meanWordLengthPropSent = 1 / numActualWords,
         uppercase = str_count(sentence, "[A-Z]"),
         lowercase = str_count(sentence, "[a-z]"),
         letters = uppercase + lowercase,
         numbers = str_count(sentence, "[0-9]"),
         spaces = str_count(sentence, " "),
         symbols = numChars - (letters + numbers + spaces)) %>%
  # convert these counts into proportions
   mutate(across(c(uppercase, lowercase, letters, numbers,
                   spaces, symbols),
                ~ .x / numChars,
                                .names = "{.col}Prop"),
          lowercasePropNonSpace = lowercase / (numChars - spaces),
          uppercasePropNonSpace = uppercase / (numChars - spaces),
          lettersPropNonSpace = letters / (numChars - spaces),
          symbolsPropNonSpace = symbols / (numChars - spaces),
          numbersPropNonSpace = numbers / (numChars - spaces),
          # create new columns converting the sentence to lowercase then removing the spaces (for some later calculations)
          sent_lower = tolower(sentence),
          sent_lower_no_space = str_replace_all(sent_lower, fixed(" "), ""))

print("---Done.")


print("Calculating the frequency that each bigram appears in the English language and averaging for each sentence.")

data_2b_bigramFreqs <- data_1_simpleCalcs %>%
    # keep only these columns
    select(sentNum, sentence, sent_lower_no_space) %>%
  # separate out the  bigrams in each word
  mutate(bigram = lapply(sent_lower_no_space, str_split_bigrams)) %>%
  unnest(bigram) %>%
  # give each bigram a bigram number
  group_by(sentNum) %>% 
  mutate(bigramNum = row_number()) %>%
  # add on the frequencies
  left_join(AllBigramsMLE,  by = c("bigram" = "Bigrams")) %>%
  mutate(n_bigrams = n(),
         n_unmatched_bigrams = sum(is.na(Frequency)),
         perc_unmatched_bigrams = n_unmatched_bigrams / n_bigrams,
         ) %>%
  ungroup()

# average the bigram frequencies
data_2_aveBigramFreqs <- data_2b_bigramFreqs %>%
  group_by(sentNum, sentence) %>%
  # summarise to collapse all individual bigram rows
  summarise(biFreqMean = mean(Frequency, na.rm = TRUE),
            propBiTop15 = mean(top15bigram, na.rm = TRUE)) %>%
  ungroup() %>%
  # join with an earlier dataset to keep the wpm info
  left_join(data_1_simpleCalcs, by = c("sentNum", "sentence"))

print("---Done.")


print("Calculating the proportion of bigrams within each hand category, e.g. hand alternation for each sentence.")

# add hands
data_3b_bigramFreqsAndHands <- data_2b_bigramFreqs %>%
  # the bigram frequency table used only has letters, so numbers and symbols will be NA in these 2 columns - this makes sure they are filled in for all character types (Pred = predecessor, first bigram character; Succ = successor, second bigram character)
  mutate(Pred = substr(bigram, 1, 1),
         Succ = substr(bigram, 2, 2)) %>%
  # separate the bigram characters onto different lines
  pivot_longer(cols = c("Pred", "Succ"),
               names_to = "characterNum",
               values_to = "character") %>%
  mutate(characterNum = recode(characterNum, "Pred" = 1, "Succ" = 2)) %>%
  # join with the hand categorisations
  left_join(select(keyboardInfo, character,
                   standardHand, standardFinger),
            by = "character") %>%
  # determine same/diff hand
  group_by(sentNum, bigramNum) %>%
  mutate(bigramHandCateg = case_when(
                       # same character
                       n_distinct(character) == 1 ~ "charRepetition",
                       # same finger, different character
                       (n_distinct(standardFinger) == 1 &&
                          n_distinct(character) == 2) ~ "fingerRepetition",
                       # same hand, different finger (and character)
                       (n_distinct(standardHand) == 1 &&
                          n_distinct(standardFinger) == 2) ~ "handRepetition",
                       # different hand (and finger and character)
                       n_distinct(standardHand) == 2 ~ "handAlternation"),
         # in case I want to add bigram hand as a predictor
         bigramHand = case_when(
                      # same hand / finger / key
                      n_distinct(standardHand) == 1 ~ 
                        str_c(standardHand, "HandBigrams"),
                      # both hands
                      n_distinct(standardHand) == 2 ~ "bothHandBigrams")) %>%
  ungroup() %>%
  group_by(sentence) %>%
  ungroup() %>%
  # get rid of some columns for now
  select(-c(characterNum, character, standardHand, standardFinger)) %>%
  # go back to one row per bigram
  unique()

# calc num of character reps, finger reps, hand reps and hand alts bigrams
data_3c_bigramHandCategTally <- data_3b_bigramFreqsAndHands %>%
  group_by(sentNum, sentence, n_bigrams, bigramHandCateg) %>%
  tally() %>%
  ungroup() %>%
  pivot_wider(names_from = bigramHandCateg,
              values_from = n,
              values_fill = 0) %>%
  # ensure all 4 columns are created even if none of the input text contains
  # them
  add_cols(c("handAlternation", "handRepetition", "fingerRepetition", "charRepetition"))


# put together the bigram frequency results and the hand alternation results
data_3_sentFreqsAlts <- left_join(data_2_aveBigramFreqs,
  data_3c_bigramHandCategTally,
                                  by = c("sentNum", "sentence")) %>%
  # calculate the bigram hand categories as proportion of the number of bigrams
  mutate(across(matches(c("charRepetition", "fingerRepetition",
                  "handRepetition", "handAlternation")),
                ~ .x / n_bigrams,
                .names = "{.col}Prop")) %>%
  select(-c("handAlternation", "handRepetition", "fingerRepetition", "charRepetition"))

print("---Done.")



print("Calculating the proportion of characters that are on the right side of the keyboard for each sentence.")

# separate out all the characters into rows
data_4b_sentPropRightHand <- data_3_sentFreqsAlts %>%
  select(sentNum, sentence) %>%
  # split the characters, and make them lowercase
  mutate(character = str_split(sentence, ""),
         character_lower = str_split(tolower(sentence), "")) %>%
  unnest(c(character, character_lower)) %>%
  # join on the standard hand (side of the keyboard) and distance from home row
  left_join(select(keyboardInfo, character, standardHand,
                   homeRowDist), by = c("character_lower" = "character")) %>%
  # save the intermediate result in a dataframe called byCharacter
  {. ->> data_4c_byCharacter } %>%
  # filter out the spaces
  filter(character != " ") %>%
  # count how many there are for each hand
  group_by(sentNum, as.factor(standardHand), .drop = FALSE) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  rename("standardHand" = "as.factor(standardHand)") %>%
  # get the proportion of each hand
  group_by(sentNum) %>%
  mutate(totNonSpace = sum(n),
            prop = n / totNonSpace) %>%
  ungroup() %>%
  # filter to right hand
  filter(standardHand == "right") %>%
  # keep only these columns
  select(sentNum, prop) %>%
  # rename the column
  rename(propRightHand = prop)

print("---Done.")


print("Calculating the distance each key is from the 'home row' and averaging for each sentence.")

# average distance from home row
data_4d_aveHomeRowDist <- data_4c_byCharacter %>%
  # filter out the spaces
  filter(character != " ") %>%
  group_by(sentNum) %>%
  summarise(aveDistFromHomeRow = mean(homeRowDist))

print("---Done.")

print("Calculating the average number of keystrokes required per word for each sentence.")

data_4e_minStrokesCalcs <- data_4c_byCharacter %>%
  group_by(sentNum) %>%
  mutate(isUppercase = if_else(str_count(character, "[A-Z]") == 1, TRUE, FALSE),
         isShiftedPunct = if_else(character %in% 
                                    c("!", "\"", "£", "$", "%", "^",
                                      "&", "*", "(", ")", "_", "+",
                                      "{", "}", ":", "@", "~", "<",
                                      ">", "?"), TRUE, FALSE),
         # within each train of capital letters, number each capital consecutively
         upperTrainSeq = if_else(isUppercase,
                                  sequence(rle(isUppercase)$lengths),
                                  as.integer(0)),
         # give each uppercase train an ID
         upperTrainID = if_else(isUppercase,
                                 cumsum(upperTrainSeq == 1),
                                 as.integer(0))) %>%
  # calculate the total number of uppercase per uppercase train
  group_by(sentNum, upperTrainID) %>%
  mutate(upperTrainLen = max(upperTrainSeq)) %>%
  ungroup() %>%
  group_by(sentNum) %>%
    # assuming here that people will use shift if it's 1-2 caps in a row and
    # caps lock of 3+ caps in a row
  mutate(numStrokesForThisChar = case_when(
    # if it's a shifted punctuation, it's 2 keystrokes (key plus shift)
    # assumption: only one in row, not !!!!, ????, :::::
    isShiftedPunct ~ 2,
    # is first of an uppercase train of any length, it's 2 keystrokes
    # (letter plus shift/caps)
    isUppercase & upperTrainSeq == 1 ~ 2,
    # is the second in an uppercase train, it's 1 keystroke
    # (shift assumed already held down)
    isUppercase & upperTrainSeq == 2 ~ 1,
    # is the non-last in an uppercase train of 3+, it's 1 keystroke
    # (shift assumed already held down)
    isUppercase & upperTrainLen >= 3 & 
      upperTrainSeq >= 3 & upperTrainSeq < upperTrainLen ~ 1,
    # if it's the last in an uppercase train of 3+, it's 2 keystrokes
    # (assumed to turn off caps lock)
    isUppercase & upperTrainLen >= 3 & (upperTrainSeq == upperTrainLen) ~ 2,
    # otherwise, 1
    TRUE ~ 1
  )) %>%
  ungroup()

data_4f_minStrokes <- data_4e_minStrokesCalcs %>%
  group_by(sentNum, sentence) %>% 
  summarise(minStrokes = sum(numStrokesForThisChar))
  

# join onto the other variables
data_4_sentIntermediate <- data_3_sentFreqsAlts %>%
  left_join(data_4b_sentPropRightHand, by = "sentNum") %>%
  left_join(data_4d_aveHomeRowDist, by = "sentNum") %>%
  left_join(data_4f_minStrokes, by = c("sentNum", "sentence")) %>%
  mutate(meanStrokesPerWord = minStrokes / numActualWords,
         meanStrokesPerChar = minStrokes / numChars)


print("---Done.")

print("Calculating the mean number of syllables per word for each sentence.")

data_5_withSylls <- data_4_sentIntermediate %>%
  mutate(meanSyllsPerWord = textstat_readability(sentence, 
                                                 "meanWordSyllables")$meanWordSyllables)

print("---Done.")


print("Calculating the proportion of words that are in the top 1000 most frequent.")


top1000WordsLemma <- read_csv(here("data", "predictor_calculation_aids", "wordFrequencyLemma.csv"),
                              show_col_types = FALSE) %>%
  filter(lemRank <= 1000)

# split the sentences into words
data_6b_byWord <- select(data_5_withSylls, sentNum, sentence) %>%
  # words are things between spaces
  mutate(word = strsplit(sentence, " ")) %>%
  unnest(word) %>%
  # do it again but with commas as sometimes there isn't a space after a comma
  mutate(word = strsplit(word, ",")) %>%
  unnest(word) %>%
  # if the "word" doesn't end in a letter or number, remove that character(s)
  mutate(word_trimmedPunct = str_replace(word,
                                        "[^A-Za-z0-9]+$",
                                        ""),
  # if the "word" doesn't start with a number of letter, remove that character(s)
         word_trimmedPunct = str_replace(word_trimmedPunct,
                                        "^[^A-Za-z0-9]+",
                                        ""),
    # lowercase word (will search for all cases - e.g. if the word is at the
    # start of a sentence so has a capital, don't only want to get the word
    # frequency of when the word has a capital- want general word frequency
    word_trimmedPunct_lower = tolower(word_trimmedPunct)) %>%
  # get rid of rows that are empty (e.g. was a double space or a comma in
  # the wrong place
  filter(word_trimmedPunct_lower != "") %>%
  group_by(sentNum) %>%
  mutate(wordNum = 1:n()) %>%
  ungroup()

data_6c_wordFreqs <- data_6b_byWord %>%
  mutate(word_trimmedPunct_lower_apostOff = case_when(
    # if the last two chars are in this list, remove them
    str_sub(word_trimmedPunct_lower, start = -2) %in% c("'s", "'m", "'d") ~ 
      str_sub(word_trimmedPunct_lower, end = -3),
    # if the last three chars are in this list, remove them
    str_sub(word_trimmedPunct_lower, start = -3) %in% c("'re", "'ve", "'ll", "n't") ~ 
      str_sub(word_trimmedPunct_lower, end = -4),
    # otherwise, leave it alone
    TRUE ~ word_trimmedPunct_lower),
    # is it in the top 1000 words (lemmas allowed)
    top1000lemm = if_else(word_trimmedPunct_lower_apostOff %in% top1000WordsLemma$word, 1, 0),
    # how many chars for the words in the top 1000?
    freqWordChars = if_else(top1000lemm == 1, nchar(word_trimmedPunct_lower), NA_real_))

data_6d_sentWordFreqs <- data_6c_wordFreqs %>%
  group_by(sentNum, sentence) %>%
  summarise(highFreqWordProp = mean(top1000lemm),
            highFreqWordCharProp = sum(freqWordChars, na.rm = TRUE) / nchar(first(sentence))) %>%
  ungroup()

data_6_withWordFreqs <- left_join(data_5_withSylls, data_6d_sentWordFreqs)

print("---Done.")


## new ##

print("Calculating the mean word frequency for each sentence.")

# read in the word freq info (from reviewer)
subtlex <- read_tsv(here("data", "predictor_calculation_aids",
                         "SUBTLEXus74286wordstextversion.txt"),
                    show_col_types = FALSE) %>%
  # convert to lowercase for case insensitivity (sentence words also converted to lower case)
  mutate(Word = tolower(Word))

# split the sentences into words
data_6e_byWord <- select(data_5_withSylls, sentNum, sentence) %>%
  # words are things between spaces
  mutate(word = strsplit(sentence, " ")) %>%
  unnest(word) %>%
  # do it again but with commas as sometimes there isn't a space after a comma
  mutate(word = strsplit(word, ",")) %>%
  unnest(word) %>%
  # clean the apostrophes
  mutate(word = str_replace_all(word,
    pattern = "[’‘`ʹʼ＇]",  replacement = "'")) %>%
  # do it again but now split along the apostrophe, to match with the word freq database
  mutate(word = strsplit(word, "'")) %>%
  unnest(word) %>%
  # if the "word" doesn't end in a letter or number, remove that character(s)
  mutate(word_trimmedPunct = str_replace(word,
                                        "[^A-Za-z0-9]+$",
                                        ""),
  # if the "word" doesn't start with a number of letter, remove that character(s)
         word_trimmedPunct = str_replace(word_trimmedPunct,
                                        "^[^A-Za-z0-9]+",
                                        ""),
    # lowercase word (will search for all cases - e.g. if the word is at the
    # start of a sentence so has a capital, don't only want to get the word
    # frequency of when the word has a capital- want general word frequency
    word_trimmedPunct_lower = tolower(word_trimmedPunct)) %>%
  # get rid of rows that are empty (e.g. was a double space or a comma in
  # the wrong place
  filter(word_trimmedPunct_lower != "") %>%
  group_by(sentNum) %>%
  mutate(wordNum = 1:n()) %>%
  ungroup()

# add on the word frequencies
data_6f_wordFreqs <- data_6e_byWord %>%
  left_join(select(subtlex, Word, FREQcount),
            by = c("word_trimmedPunct_lower" = "Word")) %>%
  mutate(FREQcount = if_else(is.na(FREQcount), 0, FREQcount))

data_6g_sentWordFreqs <- data_6f_wordFreqs %>%
  group_by(sentNum, sentence) %>%
  summarise(meanWordFreq = mean(FREQcount)) %>%
  ungroup()

data_6_withWordFreqs <- left_join(data_6_withWordFreqs, data_6g_sentWordFreqs)

print("---Done.")


print("Calculating the proportion of words that are not recognised as real words.")

data_7b_nonDictWords <- data_6b_byWord %>%
  mutate(inDictUS = hunspell_check(word_trimmedPunct, dict = dictionary("en_US")),
         inDictGB = hunspell_check(word_trimmedPunct, dict = dictionary("en_GB")),
         inDictCA = hunspell_check(word_trimmedPunct, dict = dictionary("en_CA")),
         inDictAU = hunspell_check(word_trimmedPunct, dict = dictionary("en_AU"))) %>%
  rowwise() %>%
  mutate(nonDictWord = if_else(any(inDictUS, inDictGB, inDictCA, inDictAU), 0, 1)) %>%
  ungroup() %>%
  mutate(nonDictWordChars = if_else(nonDictWord == 1, nchar(word_trimmedPunct), NA_real_))

data_7c_sentNonDictWords <- data_7b_nonDictWords %>%
  group_by(sentence) %>%
  summarise(propWordsNonDictWords = sum(nonDictWord) / n(),
        propCharsNonDictWords = sum(nonDictWordChars, na.rm = TRUE) / nchar(first(sentence)))

data_7_withNonDictWords <- left_join(data_6_withWordFreqs, data_7c_sentNonDictWords)


print("---Done.")


options(dplyr.summarise.inform = TRUE)

data_8_predictorVariables <- select(data_7_withNonDictWords,
                                    sentNum,
                                    sentence,
                                    numChars,
                                    minStrokes,
                                    numActualWords,
                                    meanStrokesPerWord,
                                    meanWordLength,
                                    meanWordLengthPropSent,
                                    highFreqWordProp,
                                    highFreqWordCharProp,
                                    meanWordFreq,
                                    propWordsNonDictWords,
                                    propCharsNonDictWords,
                                    meanSyllsPerWord,
                                    biFreqMean,
                                    propBiTop15,
                                    charRepetitionProp,
                                    fingerRepetitionProp,
                                    handRepetitionProp,
                                    handAlternationProp,
                                    lowercaseProp,
                                    uppercaseProp,
                                    numbersProp,
                                    symbolsProp,
                                    spacesProp,
                                    lowercasePropNonSpace,
                                    uppercasePropNonSpace,
                                    numbersPropNonSpace,
                                    symbolsPropNonSpace,
                                    meanStrokesPerChar,
                                    propRightHand,
                                    aveDistFromHomeRow)

return(data_8_predictorVariables)

beep()
 
}

# setup for ggpairs showing scatterplot with loess and lm in bottom half
lowerFn <-  function(data, mapping, ...){
  p <- ggplot(data = data, mapping = mapping) + 
    geom_point(alpha = 0.01) + 
    geom_smooth(method = loess, fill = "red", color = "red", ...) +
    geom_smooth(method = lm, fill = "blue", color = "blue", ...)
  p
}



```


```{r test generalisation}

# load the model
load(here("output", "typability-index-model.RData"))

# load the generalisation data
load(here("data", "generalisation", "turboTypingBeta_userData.RData"))
load(here("data", "generalisation", "turboTypingBeta_trialData.RData"))

# calculate actual typability scores
# use same code from before

crossv_z_wpm <- crossv_trial_data %>%
  group_by(tokenId) %>%
  mutate(
    mean_wpm_sent_ptp = mean(trialWPM),
    sd_wpm_sent_ptp = sd(trialWPM),
    z_wpm_sent_ptp = (trialWPM - mean_wpm_sent_ptp) / sd_wpm_sent_ptp) %>%
  ungroup() 

wpm <- crossv_z_wpm %>% 
  select(tokenId, mean_wpm_sent_ptp) %>%
  unique()
mean(wpm$mean_wpm_sent_ptp)
sd(wpm$mean_wpm_sent_ptp)

summary(wpm)

# histogram of z for each sentence

ggplot(crossv_z_wpm, aes(x = z_wpm_sent_ptp)) +
  geom_histogram() +
  facet_wrap(~sentence)




crossv_z_typability <- crossv_z_wpm %>%
  group_by(sentence) %>%
  # for each unique sentence, calculate these things
  summarise(typability_z = mean(z_wpm_sent_ptp),
            n_typists = n()) %>%
  # sort sentences alphabetically
  arrange(sentence) %>%
  # add a sentence id based on alphabetical order
  mutate(sent_id = row_number()) %>%
  # keep only these columns in this order
  select(sent_id, sentence, typability_z, n_typists)






# calc predictors

crossv_predictors <- calculatePredictorVariables(crossv_z_typability$sentence)


# Make predictions
crossv_predictions <- model2 %>% predict(crossv_predictors)





# Model performance

# plot

crossv_full <- left_join(crossv_z_typability, crossv_predictors) %>%
  cbind(crossv_predictions) %>%
  select(1:3, crossv_predictions, everything())

ggplot(crossv_full, aes(x = typability_z, y = crossv_predictions)) +
  geom_point() +
  geom_smooth(method = "lm") +
  geom_abline(slope = 1, intercept = 0)

# (a) Prediction error, RMSE
RMSE(crossv_full$crossv_predictions, crossv_full$typability_z)


# (b) R-square
R2(crossv_predictions, crossv_z_typability$typability_z)





```


---
title: "4) Test Generalisation"
author: "Emily Williams"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE) 
```

NOTE: Within the code, the test of the model's generalisation to the separate dataset is referred to as 'cross-validation', but it was later decided that 'generalisation' is a clearer term for this test.

## Load libraries

```{r load_libraries}

library(tidyverse)
library(here)
library(quanteda.textstats) # for syllable counting (more accurate than sylcount)
library(hunspell) # for spell check (is this word in the dictionary)
library(caret) # for model predictions

# function to split word/sentence into bigrams
str_split_bigrams = function(x) {
    substring(x, first = 1:(nchar(x) - 1), last = 2:nchar(x))
} 

# function to add the stated columns if they don't exist
add_cols <- function(df, cols) {
  add <- cols[!cols %in% names(df)]
  if (length(add) != 0 ) df[add] <- 0
  return(df)
}

# function to calculate the predictor variables
calculatePredictorVariables <- function(sentencesDF) {
  
  # wants only a 1d df with the first column as sentences
  
  options(dplyr.summarise.inform = FALSE)
  
  sentencesDF <- as.data.frame(sentencesDF) %>%
    rename(sentence = 1) %>%
    mutate(sentNum = row_number()) %>%
    select(sentNum, sentence)
  
# load in the helper data, e.g. bigram frequencies, hand categorisations
  
# load bigram frequencies
load(here("data", "predictor_calculation_aids", "GutenbergFrequencyTables.RData"))
rm(AllTrigramsMLE, AllLetters)

## calc percentile ranks of bigrams
AllBigramsMLE <- AllBigramsMLE %>%
  mutate(rank = rank(-Frequency, ties = "min"),
         # inflection point after first 15 top frequency bigrams in terms of their frequency (and frequency percent) - these bigrams account for 28.09% of all bigrams in English (in the corpus that Crump used)
        freqPerc = (Frequency / sum(Frequency)) * 100) %>%
  arrange(rank) %>%
  mutate(freqPercCumu = cumsum(freqPerc),
         top15bigram = if_else(rank <= 15, 1, 0))


# read in the keyboard info
keyboardInfo <- read_tsv(here("data", "predictor_calculation_aids", "keyboardInfo.txt"),
                         trim_ws = FALSE, quote = "", show_col_types = FALSE) %>%
  # for easier coding later, change the shifts to one-character symbols which are not keyboard characters in themselves
  mutate(character = case_when(character == "RSHIFT" ~ "ℛ",
                               character == "LSHIFT" ~ "ℒ",
                               TRUE ~ character))




print("Calculating the proportion of each character type (e.g. uppercase, numbers) for each sentence.")

# perform the simple calculations
data_1_simpleCalcs <- sentencesDF %>%
  # for each unique sentence, calculate these things
  mutate(numActualWords = str_count(sentence, "\\S+"),
         numChars = nchar(sentence),
         meanWordLength = numChars / numActualWords,
         # what proportion the average word takes up of the sentence
         meanWordLengthPropSent = 1 / numActualWords,
         uppercase = str_count(sentence, "[A-Z]"),
         lowercase = str_count(sentence, "[a-z]"),
         letters = uppercase + lowercase,
         numbers = str_count(sentence, "[0-9]"),
         spaces = str_count(sentence, " "),
         symbols = numChars - (letters + numbers + spaces)) %>%
  # convert these counts into proportions
   mutate(across(c(uppercase, lowercase, letters, numbers,
                   spaces, symbols),
                ~ .x / numChars,
                                .names = "{.col}Prop"),
          lowercasePropNonSpace = lowercase / (numChars - spaces),
          uppercasePropNonSpace = uppercase / (numChars - spaces),
          lettersPropNonSpace = letters / (numChars - spaces),
          symbolsPropNonSpace = symbols / (numChars - spaces),
          numbersPropNonSpace = numbers / (numChars - spaces),
          # create new columns converting the sentence to lowercase then removing the spaces (for some later calculations)
          sent_lower = tolower(sentence),
          sent_lower_no_space = str_replace_all(sent_lower, fixed(" "), ""))

print("---Done.")


print("Calculating the frequency that each bigram appears in the English language and averaging for each sentence.")

data_2b_bigramFreqs <- data_1_simpleCalcs %>%
    # keep only these columns
    select(sentNum, sentence, sent_lower_no_space) %>%
  # separate out the  bigrams in each word
  mutate(bigram = lapply(sent_lower_no_space, str_split_bigrams)) %>%
  unnest(bigram) %>%
  # give each bigram a bigram number
  group_by(sentNum) %>% 
  mutate(bigramNum = row_number()) %>%
  # add on the frequencies
  left_join(AllBigramsMLE,  by = c("bigram" = "Bigrams")) %>%
  mutate(n_bigrams = n(),
         n_unmatched_bigrams = sum(is.na(Frequency)),
         perc_unmatched_bigrams = n_unmatched_bigrams / n_bigrams,
         ) %>%
  ungroup()

# average the bigram frequencies
data_2_aveBigramFreqs <- data_2b_bigramFreqs %>%
  group_by(sentNum, sentence) %>%
  # summarise to collapse all individual bigram rows
  summarise(biFreqMean = mean(Frequency, na.rm = TRUE),
            propBiTop15 = mean(top15bigram, na.rm = TRUE)) %>%
  ungroup() %>%
  # join with an earlier dataset to keep the wpm info
  left_join(data_1_simpleCalcs, by = c("sentNum", "sentence"))

print("---Done.")


print("Calculating the proportion of bigrams within each hand category, e.g. hand alternation for each sentence.")

# add hands
data_3b_bigramFreqsAndHands <- data_2b_bigramFreqs %>%
  # the bigram frequency table used only has letters, so numbers and symbols will be NA in these 2 columns - this makes sure they are filled in for all character types (Pred = predecessor, first bigram character; Succ = successor, second bigram character)
  mutate(Pred = substr(bigram, 1, 1),
         Succ = substr(bigram, 2, 2)) %>%
  # separate the bigram characters onto different lines
  pivot_longer(cols = c("Pred", "Succ"),
               names_to = "characterNum",
               values_to = "character") %>%
  mutate(characterNum = recode(characterNum, "Pred" = 1, "Succ" = 2)) %>%
  # join with the hand categorisations
  left_join(select(keyboardInfo, character,
                   standardHand, standardFinger),
            by = "character") %>%
  # determine same/diff hand
  group_by(sentNum, bigramNum) %>%
  mutate(bigramHandCateg = case_when(
                       # same character
                       n_distinct(character) == 1 ~ "charRepetition",
                       # same finger, different character
                       (n_distinct(standardFinger) == 1 &&
                          n_distinct(character) == 2) ~ "fingerRepetition",
                       # same hand, different finger (and character)
                       (n_distinct(standardHand) == 1 &&
                          n_distinct(standardFinger) == 2) ~ "handRepetition",
                       # different hand (and finger and character)
                       n_distinct(standardHand) == 2 ~ "handAlternation"),
         # in case I want to add bigram hand as a predictor
         bigramHand = case_when(
                      # same hand / finger / key
                      n_distinct(standardHand) == 1 ~ 
                        str_c(standardHand, "HandBigrams"),
                      # both hands
                      n_distinct(standardHand) == 2 ~ "bothHandBigrams")) %>%
  ungroup() %>%
  group_by(sentence) %>%
  ungroup() %>%
  # get rid of some columns for now
  select(-c(characterNum, character, standardHand, standardFinger)) %>%
  # go back to one row per bigram
  unique()

# calc num of character reps, finger reps, hand reps and hand alts bigrams
data_3c_bigramHandCategTally <- data_3b_bigramFreqsAndHands %>%
  group_by(sentNum, sentence, n_bigrams, bigramHandCateg) %>%
  tally() %>%
  ungroup() %>%
  pivot_wider(names_from = bigramHandCateg,
              values_from = n,
              values_fill = 0) %>%
  # ensure all 4 columns are created even if none of the input text contains
  # them
  add_cols(c("handAlternation", "handRepetition", "fingerRepetition", "charRepetition"))


# put together the bigram frequency results and the hand alternation results
data_3_sentFreqsAlts <- left_join(data_2_aveBigramFreqs,
  data_3c_bigramHandCategTally,
                                  by = c("sentNum", "sentence")) %>%
  # calculate the bigram hand categories as proportion of the number of bigrams
  mutate(across(matches(c("charRepetition", "fingerRepetition",
                  "handRepetition", "handAlternation")),
                ~ .x / n_bigrams,
                .names = "{.col}Prop")) %>%
  select(-c("handAlternation", "handRepetition", "fingerRepetition", "charRepetition"))

print("---Done.")



print("Calculating the proportion of characters that are on the right side of the keyboard for each sentence.")

# separate out all the characters into rows
data_4b_sentPropRightHand <- data_3_sentFreqsAlts %>%
  select(sentNum, sentence) %>%
  # split the characters, and make them lowercase
  mutate(character = str_split(sentence, ""),
         character_lower = str_split(tolower(sentence), "")) %>%
  unnest(c(character, character_lower)) %>%
  # join on the standard hand (side of the keyboard) and distance from home row
  left_join(select(keyboardInfo, character, standardHand,
                   homeRowDist), by = c("character_lower" = "character")) %>%
  # save the intermediate result in a dataframe called byCharacter
  {. ->> data_4c_byCharacter } %>%
  # filter out the spaces
  filter(character != " ") %>%
  # count how many there are for each hand
  group_by(sentNum, as.factor(standardHand), .drop = FALSE) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  rename("standardHand" = "as.factor(standardHand)") %>%
  # get the proportion of each hand
  group_by(sentNum) %>%
  mutate(totNonSpace = sum(n),
            prop = n / totNonSpace) %>%
  ungroup() %>%
  # filter to right hand
  filter(standardHand == "right") %>%
  # keep only these columns
  select(sentNum, prop) %>%
  # rename the column
  rename(propRightHand = prop)

print("---Done.")


print("Calculating the distance each key is from the 'home row' and averaging for each sentence.")

# average distance from home row
data_4d_aveHomeRowDist <- data_4c_byCharacter %>%
  # filter out the spaces
  filter(character != " ") %>%
  group_by(sentNum) %>%
  summarise(aveDistFromHomeRow = mean(homeRowDist))

print("---Done.")

print("Calculating the average number of keystrokes required per word for each sentence.")

data_4e_minStrokesCalcs <- data_4c_byCharacter %>%
  group_by(sentNum) %>%
  mutate(isUppercase = if_else(str_count(character, "[A-Z]") == 1, TRUE, FALSE),
         isShiftedPunct = if_else(character %in% 
                                    c("!", "\"", "£", "$", "%", "^",
                                      "&", "*", "(", ")", "_", "+",
                                      "{", "}", ":", "@", "~", "<",
                                      ">", "?"), TRUE, FALSE),
         # within each train of capital letters, number each capital consecutively
         upperTrainSeq = if_else(isUppercase,
                                  sequence(rle(isUppercase)$lengths),
                                  as.integer(0)),
         # give each uppercase train an ID
         upperTrainID = if_else(isUppercase,
                                 cumsum(upperTrainSeq == 1),
                                 as.integer(0))) %>%
  # calculate the total number of uppercase per uppercase train
  group_by(sentNum, upperTrainID) %>%
  mutate(upperTrainLen = max(upperTrainSeq)) %>%
  ungroup() %>%
  group_by(sentNum) %>%
    # assuming here that people will use shift if it's 1-2 caps in a row and
    # caps lock of 3+ caps in a row
  mutate(numStrokesForThisChar = case_when(
    # if it's a shifted punctuation, it's 2 keystrokes (key plus shift)
    # assumption: only one in row, not !!!!, ????, :::::
    isShiftedPunct ~ 2,
    # is first of an uppercase train of any length, it's 2 keystrokes
    # (letter plus shift/caps)
    isUppercase & upperTrainSeq == 1 ~ 2,
    # is the second in an uppercase train, it's 1 keystroke
    # (shift assumed already held down)
    isUppercase & upperTrainSeq == 2 ~ 1,
    # is the non-last in an uppercase train of 3+, it's 1 keystroke
    # (shift assumed already held down)
    isUppercase & upperTrainLen >= 3 & 
      upperTrainSeq >= 3 & upperTrainSeq < upperTrainLen ~ 1,
    # if it's the last in an uppercase train of 3+, it's 2 keystrokes
    # (assumed to turn off caps lock)
    isUppercase & upperTrainLen >= 3 & (upperTrainSeq == upperTrainLen) ~ 2,
    # otherwise, 1
    TRUE ~ 1
  )) %>%
  ungroup()

data_4f_minStrokes <- data_4e_minStrokesCalcs %>%
  group_by(sentNum, sentence) %>% 
  summarise(minStrokes = sum(numStrokesForThisChar))
  

# join onto the other variables
data_4_sentIntermediate <- data_3_sentFreqsAlts %>%
  left_join(data_4b_sentPropRightHand, by = "sentNum") %>%
  left_join(data_4d_aveHomeRowDist, by = "sentNum") %>%
  left_join(data_4f_minStrokes, by = c("sentNum", "sentence")) %>%
  mutate(meanStrokesPerWord = minStrokes / numActualWords,
         meanStrokesPerChar = minStrokes / numChars)


print("---Done.")

print("Calculating the mean number of syllables per word for each sentence.")

data_5_withSylls <- data_4_sentIntermediate %>%
  mutate(meanSyllsPerWord = textstat_readability(sentence, 
                                                 "meanWordSyllables")$meanWordSyllables)

print("---Done.")


print("Calculating the proportion of words that are in the top 1000 most frequent.")


top1000WordsLemma <- read_csv(here("data", "predictor_calculation_aids", "wordFrequencyLemma.csv"),
                              show_col_types = FALSE) %>%
  filter(lemRank <= 1000)

# split the sentences into words
data_6b_byWord <- select(data_5_withSylls, sentNum, sentence) %>%
  # words are things between spaces
  mutate(word = strsplit(sentence, " ")) %>%
  unnest(word) %>%
  # do it again but with commas as sometimes there isn't a space after a comma
  mutate(word = strsplit(word, ",")) %>%
  unnest(word) %>%
  # if the "word" doesn't end in a letter or number, remove that character(s)
  mutate(word_trimmedPunct = str_replace(word,
                                        "[^A-Za-z0-9]+$",
                                        ""),
  # if the "word" doesn't start with a number of letter, remove that character(s)
         word_trimmedPunct = str_replace(word_trimmedPunct,
                                        "^[^A-Za-z0-9]+",
                                        ""),
    # lowercase word (will search for all cases - e.g. if the word is at the
    # start of a sentence so has a capital, don't only want to get the word
    # frequency of when the word has a capital- want general word frequency
    word_trimmedPunct_lower = tolower(word_trimmedPunct)) %>%
  # get rid of rows that are empty (e.g. was a double space or a comma in
  # the wrong place
  filter(word_trimmedPunct_lower != "") %>%
  group_by(sentNum) %>%
  mutate(wordNum = 1:n()) %>%
  ungroup()

data_6b_wordFreqs <- data_6b_byWord %>%
  mutate(word_trimmedPunct_lower_apostOff = case_when(
    # if the last two chars are in this list, remove them
    str_sub(word_trimmedPunct_lower, start = -2) %in% c("'s", "'m", "'d") ~ 
      str_sub(word_trimmedPunct_lower, end = -3),
    # if the last three chars are in this list, remove them
    str_sub(word_trimmedPunct_lower, start = -3) %in% c("'re", "'ve", "'ll", "n't") ~ 
      str_sub(word_trimmedPunct_lower, end = -4),
    # otherwise, leave it alone
    TRUE ~ word_trimmedPunct_lower),
    # is it in the top 1000 words (lemmas allowed)
    top1000lemm = if_else(word_trimmedPunct_lower_apostOff %in% top1000WordsLemma$word, 1, 0),
    # how many chars for the words in the top 1000?
    freqWordChars = if_else(top1000lemm == 1, nchar(word_trimmedPunct_lower), NA_real_))

data_6c_sentWordFreqs <- data_6b_wordFreqs %>%
  group_by(sentNum, sentence) %>%
  summarise(highFreqWordProp = mean(top1000lemm),
            highFreqWordCharProp = sum(freqWordChars, na.rm = TRUE) / nchar(first(sentence))) %>%
  ungroup()

data_6_withWordFreqs <- left_join(data_5_withSylls, data_6c_sentWordFreqs)

print("---Done.")



print("Calculating the proportion of words that are not recognised as real words.")

data_7b_nonDictWords <- data_6b_byWord %>%
  mutate(inDictUS = hunspell_check(word_trimmedPunct, dict = dictionary("en_US")),
         inDictGB = hunspell_check(word_trimmedPunct, dict = dictionary("en_GB")),
         inDictCA = hunspell_check(word_trimmedPunct, dict = dictionary("en_CA")),
         inDictAU = hunspell_check(word_trimmedPunct, dict = dictionary("en_AU"))) %>%
  rowwise() %>%
  mutate(nonDictWord = if_else(any(inDictUS, inDictGB, inDictCA, inDictAU), 0, 1)) %>%
  ungroup() %>%
  mutate(nonDictWordChars = if_else(nonDictWord == 1, nchar(word_trimmedPunct), NA_real_))

data_7c_sentNonDictWords <- data_7b_nonDictWords %>%
  group_by(sentence) %>%
  summarise(propWordsNonDictWords = sum(nonDictWord) / n(),
        propCharsNonDictWords = sum(nonDictWordChars, na.rm = TRUE) / nchar(first(sentence)))

data_7_withNonDictWords <- left_join(data_6_withWordFreqs, data_7c_sentNonDictWords)


print("---Done.")



print("Calculating the path distances for four typing strategies.")


data_8b_sentChars <- sentencesDF %>%
  mutate(character = str_split(sentence, "")) %>%
  unnest(character) %>%
  # give each char a number
  group_by(sentNum) %>% 
  mutate(charNum = row_number()) %>%
  ungroup()

data_8c_sentCharsNoSpaces <- data_8b_sentChars %>%
  filter(character != " ")

data_8d_numKeys <- nrow(keyboardInfo)

# create two lists of every keyboard character
data_8temp_character1 <- rep(keyboardInfo$character, each = data_8d_numKeys) %>% 
  as.data.frame() %>% rename(character1 = 1)
data_8temp_character2 <- rep(keyboardInfo$character, times = data_8d_numKeys) %>% 
  as.data.frame() %>% rename(character2 = 1)

# create the combos and add the key distances
data_8e_keyDists <- data.frame(data_8temp_character1, data_8temp_character2) %>%
  left_join(select(keyboardInfo, character, xKeyCentre, yKeyCentre),
            join_by("character1" == "character")) %>%
  rename(xKeyCentre1 = xKeyCentre,
         yKeyCentre1 = yKeyCentre) %>%
  left_join(select(keyboardInfo, character, xKeyCentre, yKeyCentre),
            join_by("character2" == "character")) %>%
  rename(xKeyCentre2 = xKeyCentre,
         yKeyCentre2 = yKeyCentre) %>%
  # calculate the distances between each key pair
  mutate(bigram = paste(character1, character2, sep = ""),
         keyDist = sqrt((xKeyCentre2 - xKeyCentre1)^2 + (yKeyCentre2 - yKeyCentre1)^2),
         keyDistMM = keyDist * 19.05) %>%
  select(-c(xKeyCentre1, xKeyCentre2, yKeyCentre1, yKeyCentre2))

rm(data_8temp_character1, data_8temp_character2)





# this is a dataframe that will at the end just be the shifts, and will be binded (bound?) together with the sent Chars
data_8f_sentCharsShifts <- data_8c_sentCharsNoSpaces %>%
  # add on standard hand and shift
  left_join(select(keyboardInfo, character, standardHand, shiftNeeded),
            by = join_by(character)) %>%
  # filter to just shift needed
  filter(shiftNeeded == TRUE) %>%
  # assign a character number for the shift - to be before the key that needs to be shifted
  mutate(charNum = charNum - 0.5,
         # also set the character as the opposite shift
         character = if_else(standardHand == "left", "ℛ", "ℒ")) %>%
  select(-c(standardHand, shiftNeeded))

# bind and order
data_8g_sentCharsNoSpacesWithShifts <- rbind(data_8c_sentCharsNoSpaces, data_8f_sentCharsShifts) %>%
  arrange(sentNum, charNum)




# single path = add up all distances, assume other hand for shift 


data_8h_onePath <- sentencesDF %>%
  # remove the spaces (assume typed with thumb)
  mutate(sentenceNoSpaces = str_replace_all(sentence, " ", "")) %>%
  # separate out the  bigrams
  mutate(bigram = lapply(sentenceNoSpaces, str_split_bigrams)) %>%
  unnest(bigram) %>%
  # give each bigram a bigram number
  group_by(sentNum) %>% 
  mutate(bigramNum = row_number()) %>%
  ungroup() %>%
  # add key distances
  left_join(select(data_8e_keyDists, bigram, keyDist), by = join_by(bigram))

data_8i_onePathSumm <- data_8h_onePath %>%
  group_by(sentNum) %>%
  summarise(sentence = first(sentence),
            #pathText = first(pathText),
            pathLength = sum(keyDist)) %>%
  ungroup() %>%
  group_by(sentNum) %>%
  mutate(totalSinglePathLength = sum(pathLength, na.rm = TRUE)) %>%
  ungroup()

data_8j_onePathTotals <- select(data_8i_onePathSumm, -pathLength)






# two paths =
# group by standard hand
# determine bigrams
# add up distances

data_8k_twoPaths <- data_8g_sentCharsNoSpacesWithShifts %>%
  # add on other cols (standard hand)
  left_join(select(keyboardInfo, character, standardHand),
            by = join_by(character)) %>%
  # determine the characters in each path (hand)
  group_by(sentNum, standardHand) %>%
  mutate(pathText = paste0(character, collapse = "")) %>%
  # remove some columns and get to one row per path (hand)
  ungroup() %>%
  select(-c(character, charNum)) %>%
  unique() %>%
  # separate out the bigrams of each path
  mutate(bigram = lapply(pathText, str_split_bigrams)) %>%
  unnest(bigram) %>%
  # group by standard hand
  group_by(sentNum, standardHand) %>% 
  mutate(bigramNum = row_number()) %>%
  ungroup() %>%
  # add distances
  left_join(select(data_8e_keyDists, bigram, keyDist), by = join_by(bigram))

data_8l_twoPathsSumm <- data_8k_twoPaths %>%
  group_by(sentNum, standardHand) %>%
  # for each standard hand, determine the path and sum the distances
  summarise(sentence = first(sentence),
            pathText = first(pathText),
            pathLength = sum(keyDist)) %>%
  ungroup() %>%
  group_by(sentNum) %>%
  mutate(totalDualPathLength = sum(pathLength, na.rm = TRUE)) %>%
  ungroup()

data_8m_twoPathsTotals <- data_8l_twoPathsSumm %>%
  select(sentNum, sentence, totalDualPathLength) %>%
  unique()



# four paths =
# group by standard finger
# change fingers to supposed fingers
# determine bigrams
# add up distances

data_8n_fourPaths <- data_8g_sentCharsNoSpacesWithShifts %>%
  # add on other cols (standard finger)
  left_join(select(keyboardInfo, character, standardFinger),
            by = join_by(character)) %>%
  # determine the supposed finger
  mutate(supposedFinger = case_when(
    standardFinger %in% c(0, 1) ~ 2,
    standardFinger %in% c(2, 3) ~ 3,
    standardFinger == 5 ~ 5,
    standardFinger %in% c(6, 7) ~ 6,
    standardFinger %in% c(8, 9) ~ 7)) %>%
  select(-standardFinger) %>%
  group_by(sentNum, supposedFinger) %>%
  mutate(pathText = paste0(character, collapse = "")) %>%
  ungroup() %>%
  select(-c(character, charNum)) %>%
  unique() %>%
  # separate out the  bigrams
  mutate(bigram = lapply(pathText, str_split_bigrams)) %>%
  unnest(bigram) %>%
  # group by standard finger
  group_by(sentNum, supposedFinger) %>% 
  mutate(bigramNum = row_number()) %>%
  ungroup() %>%
  # add distances
  left_join(select(data_8e_keyDists, bigram, keyDist), by = join_by(bigram))
  
data_8o_fourPathsSumm <- data_8n_fourPaths %>%
    group_by(sentNum, supposedFinger) %>%
    # for each standard finger, determine the path and sum the distances
    summarise(sentence = first(sentence),
              pathText = first(pathText),
              pathLength = sum(keyDist)) %>%
    ungroup() %>%
    group_by(sentNum) %>%
    mutate(totalQuadPathLength = sum(pathLength, na.rm = TRUE)) %>%
  ungroup()
  
data_8p_fourPathsTotals <- data_8o_fourPathsSumm %>%
    select(sentNum, sentence, totalQuadPathLength) %>%
    unique()
  


# touch typing = 
# group by standard finger
# then same as above

data_8q_touchPaths <- data_8g_sentCharsNoSpacesWithShifts %>%
  # add on other cols (standard finger)
  left_join(select(keyboardInfo, character, standardFinger),
            by = join_by(character)) %>%
  # determine the characters in each path (finger)
  group_by(sentNum, standardFinger) %>%
  mutate(pathText = paste0(character, collapse = "")) %>%
  # remove some columns and get to one row per path (hand)
  ungroup() %>%
  select(-c(character, charNum)) %>%
  unique() %>%
  # separate out the  bigrams for each path
  mutate(bigram = lapply(pathText, str_split_bigrams)) %>%
  unnest(bigram) %>%
  # group by standard finger
  group_by(sentNum, standardFinger) %>% 
  mutate(bigramNum = row_number()) %>%
  ungroup() %>%
  # add distances
  left_join(select(data_8e_keyDists, bigram, keyDist), by = join_by(bigram))

data_8r_touchPathsSumm <- data_8q_touchPaths %>%
  group_by(sentNum, standardFinger) %>%
  # for each standard finger, determine the path and sum the distances
  summarise(sentence = first(sentence),
            pathText = first(pathText),
            pathLength = sum(keyDist)) %>%
  ungroup() %>%
  group_by(sentNum) %>%
  mutate(totalOctoPathLength = sum(pathLength, na.rm = TRUE)) %>%
  ungroup()

data_8s_touchPathsTotals <- data_8r_touchPathsSumm %>%
  select(sentNum, sentence, totalOctoPathLength) %>%
  unique()


# combine totals
data_8t_pathTotals <- left_join(data_8j_onePathTotals, data_8m_twoPathsTotals,
                                by = join_by(sentNum, sentence)) %>%
    left_join(data_8p_fourPathsTotals, by = join_by(sentNum, sentence)) %>%
    left_join(data_8s_touchPathsTotals, by = join_by(sentNum, sentence))
  
# add to other predictors df

data_8_withTotalPathLengths <- left_join(data_7_withNonDictWords, data_8t_pathTotals) 



options(dplyr.summarise.inform = TRUE)

data_9_predictorVariables <- select(data_8_withTotalPathLengths,
                                    sentNum,
                                    sentence,
                                    numChars,
                                    minStrokes,
                                    numActualWords,
                                    meanStrokesPerWord,
                                    meanWordLength,
                                    meanWordLengthPropSent,
                                    highFreqWordProp,
                                    highFreqWordCharProp,
                                    propWordsNonDictWords,
                                    propCharsNonDictWords,
                                    meanSyllsPerWord,
                                    biFreqMean,
                                    propBiTop15,
                                    charRepetitionProp,
                                    fingerRepetitionProp,
                                    handRepetitionProp,
                                    handAlternationProp,
                                    lowercaseProp,
                                    uppercaseProp,
                                    numbersProp,
                                    symbolsProp,
                                    spacesProp,
                                    lowercasePropNonSpace,
                                    uppercasePropNonSpace,
                                    numbersPropNonSpace,
                                    symbolsPropNonSpace,
                                    meanStrokesPerChar,
                                    propRightHand,
                                    aveDistFromHomeRow,
                                    totalSinglePathLength,
                                    totalDualPathLength,
                                    totalQuadPathLength,
                                    totalOctoPathLength)

return(data_9_predictorVariables)

beep()
 
}



# setup for ggpairs showing scatterplot with loess and lm in bottom half
lowerFn <-  function(data, mapping, ...){
  p <- ggplot(data = data, mapping = mapping) + 
    geom_point(alpha = 0.01) + 
    geom_smooth(method = loess, fill = "red", color = "red", ...) +
    geom_smooth(method = lm, fill = "blue", color = "blue", ...)
  p
}



```


```{r test generalisation}

# load the model
load(here("output", "typability-index-model.RData"))

# load the generalisation data
load(here("data", "generalisation", "turboTypingBeta_userData.RData"))
load(here("data", "generalisation", "turboTypingBeta_trialData.RData"))

# calculate actual typability scores
# use same code from before

crossv_z_wpm <- crossv_trial_data %>%
  group_by(tokenId) %>%
  mutate(
    mean_wpm_sent_ptp = mean(trialWPM),
    sd_wpm_sent_ptp = sd(trialWPM),
    z_wpm_sent_ptp = (trialWPM - mean_wpm_sent_ptp) / sd_wpm_sent_ptp) %>%
  ungroup() 

wpm <- crossv_z_wpm %>% 
  select(tokenId, mean_wpm_sent_ptp) %>%
  unique()
mean(wpm$mean_wpm_sent_ptp)
sd(wpm$mean_wpm_sent_ptp)

summary(wpm)

# histogram of z for each sentence

ggplot(crossv_z_wpm, aes(x = z_wpm_sent_ptp)) +
  geom_histogram() +
  facet_wrap(~sentence)




crossv_z_typability <- crossv_z_wpm %>%
  group_by(sentence) %>%
  # for each unique sentence, calculate these things
  summarise(typability_z = mean(z_wpm_sent_ptp),
            n_typists = n()) %>%
  # sort sentences alphabetically
  arrange(sentence) %>%
  # add a sentence id based on alphabetical order
  mutate(sent_id = row_number()) %>%
  # keep only these columns in this order
  select(sent_id, sentence, typability_z, n_typists)






# calc predictors

crossv_predictors <- calculatePredictorVariables(crossv_z_typability$sentence)


# Make predictions
crossv_predictions <- model2 %>% predict(crossv_predictors)





# Model performance

# plot

crossv_full <- left_join(crossv_z_typability, crossv_predictors) %>%
  cbind(crossv_predictions) %>%
  select(1:3, crossv_predictions, everything())

ggplot(crossv_full, aes(x = typability_z, y = crossv_predictions)) +
  geom_point() +
  geom_smooth(method = "lm") +
  geom_abline(slope = 1, intercept = 0)

# (a) Prediction error, RMSE
RMSE(crossv_full$crossv_predictions, crossv_full$typability_z)


# (b) R-square
R2(crossv_predictions, crossv_z_typability$typability_z)





```

